\chapter{Background and Related Work}
\label{related_work}

\section{Transactional Memory}
A transaction, or a group of operations that together form one logical unit of work, allows a programmer to more simply reason about concurrent access to shared state. The concept developed first in database theory, trickled into file systems, and expanded to other domains with the development of hardware transactional memory (HTM) in 1986 and software transactional memory (STM) in 1995~\cite{harristm}. 
A transaction is commonly defined by the ACID properties: atomicity, consistency, isolation, and durability. Transactional memory (TM)~\cite{harristm, herlihytm} guarantees transactional properties in shared memory; this differs from transactions in a database which (traditionally) work with data on disk.
TM is therefore unconcerned with the durability property: memory does not persist on some permanent medium. However, TM must still adhere to the remaining three ACI properties:

\emph{Atomicity} ensures that if a transaction commits, all changes made by the transaction are instantly visible to other actors performing transactions (for example, threads modifying shared memory or processes modifying a database); if a transaction aborts, none of the changes made by the transaction are visible to any other actor. Either all or none of the operations of the transaction should appear to succeed.

\emph{Consistency} guarantees that a transaction begins and ends with the state of the database or data structure satisfying particular (data structure-specific) invariants: for example, an invariant could be that the data structure is left in a state without duplicate entries.

\emph{Isolation} ensures that a transaction's execution appears to be isolated from any other transaction's (possibly simultaneous) execution. This allows for transactions to be \emph{serializable}, which means that one can find an ordering of committed transactions that satisfies the observed history of results. In this ordering, it should appear as if operations within one transaction are never interleaved with operations in another transaction. 

Transactional memory transactions also are \emph{linearizable}~\cite{linearizability}: all transactions performed at a later clock time than a committed transaction observe the changes made by the committed transaction. This allows programmers to easily determine the order and effects of transactions.

\subsection{Transaction Execution and Commit Protocol}
To use a transactional system, a programmer indicates the start and end of the transaction. When the transaction finishes, it runs a commit protocol. The basic commit protocol has four phases~\cite{harristm}:
\begin{enumerate}
    \item Lock: Acquire the necessary locks to ensure protected access to any state the transaction intends to modify.
    \item Check: Any state observed by the transaction must be checked to ensure that the state has not changed in a way that would invalidate the results of the transaction. If the check fails, the transaction aborts. If the transaction passes the check phase, the transaction will commit at this time.
    \item Install: All modifications of the transaction are installed.
    \item Cleanup: Release the locks of any state modified by the transaction. If the transaction aborts during Phase 2 (check) or at any time during the transaction's execution, cleanup also removes any changes that the transaction may have made to the state.
\end{enumerate}

The exact details of the commit protocol depend on whether the transactional memory system is optimistic, pessimistic, or some combination of the two, and whether it uses eager and/or lazy updates~\cite{harristm}. 
Optimistic transactional memory systems and pessimistic transactional memory systems both track the transaction's intended changes in the transaction's \emph{write set} and any state observed during the transaction in the transaction's \emph{read set}. 
An optimistic system checks for invalidated values in the read set only at commit time. If all values are valid, the system performs the changes in the transaction's write set and the transaction is marked committed. Otherwise, the transaction aborts and the system ensures that the transaction leaves no visible effects (potentially rolling back any changes the transaction has made). An optimistic system therefore assumes (optimistically) that no other thread will conflict with another thread's transaction, and executes all operations of the transaction before it checks if any conflict has indeed occurred.

A pessimistic transactional system instruments every read and write with additional checks to see if another thread is simultaneously accessing the same part of memory and creating a conflict. If any such conflicts are detected during the read or write, then the thread either aborts or stalls until the conflicting transaction completes by either committing or aborting. A pessimistic approach therefore assumes that multiple threads will often be modifying or reading memory at the same locations during their transactions' executions. 

Transactional memory systems also practice eager updates or lazy updates. A system is eager when it updates shared memory during the transaction's execution. It maintains an ``undo'' log of changes, which is used if the transaction aborts and the changes need to be undone. 
A system is lazy when no changes are done until the transaction commits: all intended changes during execution are buffered by the system or written to a log and applied at commit time. Systems can range from fully eager to fully lazy, with most practicing a mix of the two techniques.

Transactional systems can also provide the \emph{opacity} property~\cite{opacity}, a property that guarantees that the code running transactions will never observe an inconsistent memory state. 
In other words, all the transaction's reads must be consistent with a single snapshot of memory at some point during the transaction.
Opacity is necessary to avoid potential failures such as infinite loops and invalid pointer dereferences. With opacity, a transaction will abort immediately upon observing an inconsistent state that would cause the transaction to fail at commit time. Note that pessimism does not guarantee opacity: a transaction performing read-only operations will not acquire locks on the data structure, and may therefore observe an inconsistent state at some point during its execution.
In certain systems such as STO, providing opacity requires keeping track of a global transaction ID (TID). This global TID is used to check the consistency of previously read items when the transaction observes a memory state. The TID is updated whenever a transaction commits and modifies memory. Opacity hinders the performance of the code running transactions because this TID must be accessed whenever a transaction accesses memory (to check the state of previously read items) and when a transaction commits. In our benchmarks, we disable opacity---allowing for the possibility of rare failures caused by memory inconsistencies---for the sake of performance. We benchmark transactional data structures without opacity enabled to measure our transactional system at its maximum achievable performance.

\section{Transactional Memory Systems}
Transactional Memory can be implemented in both hardware and software. Although hardware transactional memory (HTM) naturally outperforms software transactional memory (STM), a purely hardware TM has several inherent limitations. HTM will fail when the working sets of the transactions exceed hardware capacities; for example, the buffer used to track read and writes of the transaction is restricted in size. In addition, HTM lacks flexibility because the granularity of reads/writes is at the word level, and fails to be truly composable~\cite{htm}. Nevertheless, with the increasing support for HTM in computer hardware~\cite{intel_htm}, integrating STM with HTM offers performance improvements over what STM alone can achieve.

The STO system~\cite{sto} used by this work is one of several STM systems. For simplicity, we can generalize STMs into three groups: word/object-based STMs, which track individual memory words or objects touched during a transaction, STMs that expose non-transactional API for performance gains, and abstraction-based STMs which track items based on abstract datatypes.

TL2~\cite{tl2} and LarkTM~\cite{larktm} are highly-optimized word-STMs that track memory words touched during a transaction. SwissTM~\cite{swisstm} is also a word-STM, but increases performance by tracking memory in 4-word groups, resulting in less overhead than tracking individual memory words. There have also been object-based STMs which track objects instead of memory words, but incur extra cost by shadow copying any objects written to within the transaction~\cite{stm_objects}. 

Open nesting~\cite{opennesting}, elastic transactions~\cite{elastic}, transactional collection classes~\cite{tcc}, early release~\cite{earlyrelease}, and SpecTM~\cite{spectm} are techniques for implementing transactions that, like STO, speed up transactional performance by reducing the bookkeeping costs and the number of false conflicts. However, these techniques expose non-transactional APIs to the programmer, which requires the programmer to be an expert in the system in order to implement transactions. This complicates, rather than simplifies, concurrent programming. For example, transactional collection classes remove unnecessary memory conflicts in data structures by wrapping the datatype with semantic locks; this requires designing multi-level, open-nested transactions, and presents a much more complicated framework than does STO, which allows the datatype to be designed specifically to support transactions.

STO is one of several STM systems that use abstraction to improve STM performance~\cite{predication,autolock, optboost, boost}. These systems expose a transactional API to programmers in the form of concurrent, transactional data structures that are written on top of an STM. However, systems other than STO build their data structures on top of traditional word-STMs, whereas STO builds data structures on top of an abstract STM which tracks abstract items defined by each data structure. This lets STO improve performance beyond that of previous systems. 

Our work focuses on datatypes and how they perform within transactional settings. We now take a closer look at STO and other abstract STM systems that expose an API in the form of transactional data structures allowing programmers to use transactions.

\section{Abstraction-based STMs}

The STO system tracks the abstract operations of a transactional abstract datatype. STO consists of a core system that implements a transactional, optimistic commit-time protocol, and an extensible library of transactional datatypes built on top of this core. While programmers can use the many transactional datatypes already implemented in STO (ranging from queues to red-black trees), STO provides a transactional framework that allows programmers to easily add transactional support to other datatypes based on their particular semantics. STO allows datatypes to add datatype-specific \emph{items} to their read or write sets, thereby reducing bookkeeping costs by 
exploiting the semantics of the particular datatype. 

Many datatypes enforce transactional correctness using \emph{versions} that correspond to items. A version acts as a lock on the data structure: in order to update the data structure, a thread must first lock the version. A version tracks changes to the data structure by monotonically increasing whenever a thread modifies the corresponding data structure item. Thus, any version seen by a thread is equivalent to some previous or current state of the data structure. When a transactional operation performs a read of some data structure state, it adds a read of the corresponding version value. This read version value is checked when the transaction commits to ensure that the observed data structure state is still valid: if the version value has changed, then the data structure state has changed as well.
The version's value from the \emph{first} time we perform a read of the version is validated at commit time; this ensures that every observed state since the start of the transaction is still valid.

STO also allows datatypes to use a variety of different concurrency control algorithms. For example, STO's transactional hashmap defines abstract items for each bucket, which are invalidated only when the bucket size changes. This means that transactions conflict only when modifying or reading the same bucket, allowing scalable access to the data structure. In addition, at most one item is added to the read- or write-set per operation, which can be orders of magnitude fewer than the number of items tracked by a word- or object-based STM. STO allows datatypes to define their own strategies for transaction execution: a datatype can insert elements during transaction execution (eagerly) or wait until commit time to insert (lazily). The specifics of the commit protocol are implemented as datatype callbacks. This allows a datatype to use pessimistic strategies for certain operations (e.g., by needing to acquire a lock) while using optimistic strategies for other operations. STO is therefore a flexible hybrid of optimistic/pessimistic and eager/lazy versioning strategies.

Boosting~\cite{boost} is a method to convert highly-concurrent (non-transactional) data structures into transactional data structures. Like STO, boosting determines conflicts between transactions by relying on a particular data structure's semantics. Instead of allowing each datatype to define read- and write-set items, however, boosting maps a datatype's operations to abstract locks. If two operations do not commute---i.e., swapping the order of their invocations affects the final state of the data structure or the responses returned by the operations---then the abstract locks for the operations will conflict: for one of the operations to be performed, both the abstract lock for that operation and the abstract lock for the conflicting operation must be acquired.
Thus, the granularity of synchronization of the original linearizable data structure is only achievable for commutative operations in its boosted form. Because a transaction may abort, boosting practices undo logging and requires that each operation has an inverse. These constraints mean that boosting is not applicable to all data structures, but make it particularly useful for ones like sets. Unlike boosting, STO allows us to work with data structures whose operations have no inverse. Boosting is an inherently pessimistic strategy, using eager versioning, whereas STO allows for a hybrid approach that can improve performance.

Optimistic boosting~\cite{optboost} is a technique meant to improve the performance of boosting. It proposes an optimistic approach in which acquisition of the abstract locks is delayed until commit time. During execution, semantic items are added to the read- and write-sets of the transaction and validated at commit time; if all reads are valid, then the appropriate abstract locks are acquired and modifications applied to the data structure. Because execution is optimistic and abstract locks are not eagerly acquired at the higher, ``boosted'' level, the underlying concurrent data structure can be lazy and wait until commit time to execute operations. This adds support for operations that may not have an inverse. 
However, optimistic boosting has not been shown to be effective in practice. STO provides a more flexible hybrid transactional framework and outperforms optimistic boosting.

Automated locking~\cite{autolock} takes a similar approach to boosting by pessimistically acquiring abstract locks corresponding to each operation. It differs from boosting because it also takes a datatype-specific commutativity specification of conditions in which operations commute. The commutativity specification of an abstract datatype is compiled into a symbolic set (called a ``locking mode'') that is used to prevent conflicting operations from being run concurrently. If two operations commute, the ``locking mode'' allows them to run concurrently.
This approach optimizes and automates the creation of abstract locks for an abstract data structure. A similar approach may help STO data structures choose which abstract read and write items to track during a transaction to avoid false conflicts.

Predication~\cite{predication} is a technique that maps operations to ``predicate words'' contained in a shared-memory table managed by the STM. Each predicate word represents a property of the data structure. For example, looking up an element $e$ in a set would be associated with an \texttt{\{in\_set?($e$)\}} predicate word, and the STM would add reads or writes of the predicate word when $e$ is removed or added to the set. 
Unlike boosting, transactional predication achieves semantic conflict detection without keeping an undo log. However, predication must insert predicate words into the table for absent as well as present lookups, causing a garbage collection problem that STO and other systems do not face. Transactional predication also focuses on making transactional data structures perform equally as well as highly-concurrent data structures in non-transactional settings. This is orthogonal to our work here and can be a future line of optimization for STO data structures.

The Transactional Data Structures Libraries (TDSL)~\cite{tdsl}, like STO, offer collections of transactional data structures for programmers to use. Similar to STO, each TDSL data structures executes transactions with a customized mix of pessimistic, optimistic, eager, and lazy strategies.
This allows for optimizations that rely on the specific data structure's semantics. Unlike STO, data structures in TDSL are also optimized for single-operation transactions (which we see as an orthogonal line of work). While TDSL contains only a queue and skiplist, STO has implementations of many other data structures in transactional settings, including a hashmap, list, priority queue, and red-black tree. Our work in this thesis draws upon some of the algorithm designs for the queue implemented in TDSL.

This thesis investigates the integration of several highly-concurrent (non-transactional) data structures with STO. In particular, we test and modify different concurrent queue and concurrent hashmap algorithms, which we describe in more detail in Chapters~\ref{queue} and~\ref{hashmap}. Similar work has been done with lazy sets~\cite{lazyset}, in which transactional support is added to a lazy concurrent set. We extend this work to other data structures and investigate further how to achieve performance close to highly concurrent, non-transactional data structures.

\subsection{Commutativity in Transactional Data Structures}

STO, along with the above methods of integrating concurrent abstract datatypes with STM, builds upon the ideas introduced by Weihl in the late 1980s~\cite{weihl}. Weihl defines optimal local atomicity properties of datatypes that allow for transactions to be serialized, therefore providing an upper bound on the amount of concurrency achievable in transactional datatypes. These local properties are derived from algebraic properties of the data structure, such as commutativity of particular operations. Weihl also demonstrates an inherent relation between commutativity-based concurrency control and transactional recovery algorithms. Unlike our work with STO, however, Weihl does not focus on the implementation of these properties.

Schwarz and Spector~\cite{schwarz} introduce a theory for ordering concurrent transactions based on the semantics of shared abstract datatypes and dependencies of different datatype operations. For each operation, the programmer specifies the operation's preconditions, postconditions, and invariants. To describe interactions between operations in a transaction, the programmer additionally provides an interleaving specification. This defines dependency relations between datatype operations. From this set of dependency relations, Schwarz and Spector can define the limits of concurrency for the datatype by drawing upon results from Korth~\cite{korth} which show that when two operations commute (have no inter-dependencies), the order in which the operations are ordered do not affect serializability. We discuss this work further in Chapter \ref{commutativity}. 

Schwarz and Spector also demonstrate that increased concurrency can be achieved by weakening the serializability of transactions: once the semantics of a datatype have been taken into account, the remaining constraints on concurrency come from enforcing transactional properties~\cite{kung}. Weaker ordering properties allow a datatype to fully exploit its operations' semantics. They exemplify this with a WQueue design: a higher-concurrency queue with modified semantics that preserves a weaker ordering property than serializability. However, their paper focuses on the theoretical result instead of the implementation and is not concerned with explicit concurrent algorithms for the queue. Our work in Chapter \ref{commutativity} with the weakly-transactional queue builds off this idea of a weaker transactional specification, and we provide experimental evidence that operation semantics can be usefully exploited only by providing weaker transactional guarantees.

Badrinath and Ramamritham~\cite{badrinath} investigate operation commutativity to define recoverability, a weaker notion of commutativity that can achieve enhanced concurrency. Their key observation is that recoverability, unlike commutativity, takes into account the \emph{removal} of operations from the execution history. If an operation is recoverable with respect to another transaction's operation, then the operation can be eagerly applied to the data structure without the other transaction's operation aborting or committing. Although this technique forces an ordering at commit time, it has the nice property that at least one of the transactions will be able to commit. This prevents issues such as cascading aborts, in which one transaction's abort causes the abort of another transaction, which can cause the abort of a third transaction (and so on).

Kulkarni et.al.~\cite{galois} define the notion of a commutativity lattice (predicates between pairs of methods) to reason about commutativity in a data structure. The Galois system, upon which this idea is tested, provides a framework in which the programmer defines a commutativity lattice for individual data structures and, by exploiting commutativity, improves the performance of irregular parallel applications. Galois, however, focuses on constructing commutativity checkers instead of serializing transactions.

Commutativity work has also played a large part in optimizing distributed transactions. Mu, et.al.~\cite{distributed} introduce a system, ROCOCO, that first distributes pieces of concurrent transactions across multiple servers. These servers then determine dependencies between their pieces of concurrent transactions based on the commutativity of operations in the transactions.
 Transaction execution is delayed until commit time, when its corresponding dependency information is sent to all servers via a coordinator, allowing the servers to re-order conflicting pieces of the transaction and execute them in a serializable order. This reduces aborts and unnecessary conflicts.
Finally, commutativity has also been explored in network consistency algorithms and conflict-free replicated data types (CDRTs)~\cite{CRDT}.
