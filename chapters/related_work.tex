\chapter{Background and Related Work}
\label{related_work}

\section{Transactional Memory}
A transaction, or a group of operations, allows a programmer to more simply reason about concurrent access to shared state. The concept developed first in database theory~\cite{harristm}, trickled into file systems, and expanded to other domains with the development of hardware transactional memory (HTM) in 1986 and software transactional memory (STM) in 1995. 
A transaction is commonly defined by the ACID properties: atomicity, consistency, isolation, and durability. Transactional memory~\cite{harristm}~\cite{herlihytm} guarantees transactional properties in shared memory; this differs from transactions in a database which (traditionally) work with data on disk.
The durability property is irrelevant in transactional memory because memory does not persist on some permanent medium. However, the remaining three ACI properties provide important guarantees:

\emph{Atomicity} ensures that if a transaction commits, all changes made by the transaction are instantly visible to other actors performing transactions (for example, threads modifying shared memory or processes modifying a database); if a transaction aborts, none of the changes made by the transaction are visible to any other actor. Either all or none of the operations of the transaction should appear to succeed.

\emph{Consistency} guarantees that a transaction begins and ends with the state of the database or data structure satisfying particular (data structure-specific) invariants: for example, an invariant could be that the data structure is left in a state without duplicate entries.

\emph{Isolation} ensures that a transaction's execution appears to be isolated from any other transaction's (possibly simultaneous) execution. This allows for transactions to be \emph{serializable}, which means that one can find an ordering of committed transactions that satisfies the observed history of results. In this ordering, it should appear as if operations within one transaction are never interleaved with operations in another transaction. 

Transactional memory transactions also are \emph{linearizable}~\cite{linearizability}: all transactions performed at a later clock time than a committed transaction observe the changes made by the committed transaction. This allows programmers to easily determine the order and effects of transactions.


Systems implementing transactional memory can be optimistic, pessimistic, or some combination of the two, and may use eager and/or lazy updates. 
Optimistic transactional memory systems and pessimistic transactional memory systems~\cite{harristm} both track any intended changes to be made during the transaction in the transaction's \emph{write set} and any state observed during the transaction in the transaction's \emph{read set}. 
An optimistic system checks for invalidated values in the read set only at commit time. If all values are valid, the system performs the changes in the transaction's write set and the transaction is marked committed. Otherwise, the transaction aborts and the system ensures that the transaction leaves no visible effects (potentially rolling back any changes the transaction has made). An optimistic system therefore optimistically believes that no other thread will conflict with another thread's transaction, and executes all operations of the transaction before it checks if any conflict has indeed occurred.

A pessimistic transactional system instruments every read and write with additional checks to see if another thread is simultaneously accessing the same part of memory and creating a conflict. If any such conflicts are detected during the read or write, then the thread either aborts or stalls until the conflicting transaction completes by either committing or aborting. This pessimistic approach therefore assumes that multiple threads will often be modifying or reading memory at the same locations during their transactions' executions. 

Transactional memory systems also practice eager updates or lazy updates. A system is eager when it updates shared memory during the transaction's execution. It maintains an ``undo'' log of changes, which is used if the transaction aborts and the changes need to be undone. A system is lazy when no changes are done until the transaction commits: all intended changes during execution are buffered by the system or written to a log and applied at commit time. Systems can range from fully eager to fully lazy, with most practicing a mix of the two techniques.

Transactional systems can also provide the \emph{opacity} property~\cite{opacity}, a property that guarantees that the code running transactions will never observe an inconsistent memory state. 
In other words, all the transaction's reads must be consistent with a single snapshot of memory at some point during the transaction.
Opacity is necessary to avoid potential failures such as infinite loops and invalid pointer dereferences. With opacity, a transaction will abort immediately upon observing an inconsistent state that would cause the transaction to fail at commit time. Note that pessimism does not guarantee opacity: two threads may modify the same parts of memory at different points during their transactions and therefore avoid simultaneous access conflicts.
In some systems (including the one used in this work), providing opacity requires keeping track of a global transaction ID (TID). This global TID is used to check the consistency of previously read items when the transaction observes a memory state. The TID is updated whenever a transaction commits and modifies memory. Opacity hinders the performance of the code running transactions because this TID must be accessed whenever a transaction accesses memory (to check the state of previously read items) and when a transaction commits. In our benchmarks, we disable opacity---allowing for the possibility of rare failures caused by inconsistencies---for the sake of performance. We benchmark transactional data structures without opacity enabled to measure our transactional system at its maximum achievable performance.

\section{Transactional Memory Systems}
Transactional Memory can be implemented in both hardware and software. Although hardware transactional memory naturally outperforms software transactional memory, a purely hardware TM has several inherent limitations. HTM will fail when the working sets of the transactions exceed hardware capacities; for example, the buffer used to track read/writes of the transaction is restricted in size. In addition, HTM lacks flexibility (granularity of reads/writes is at the word level) and fails to be truly composable~\cite{htm}. Nevertheless, with the increasing support for HTM in computer hardware~\cite{intel_htm}, integrating STM with HTM offers potential performance improvements over what STM can achieve alone.

The STO system~\cite{sto} used by this work is one of several STM systems. For simplicity, we can generalize STMs into three groups: word/object-based STMs, which track individual memory words or objects touched during a transaction, STMs that expose non-transactional API for performance gains, and abstraction-based STMs which track items based on abstract datatypes.

TL2~\cite{tl2} and LarkTM~\cite{larktm} are highly-optimized word-STMs that track memory words touched during a transaction. SwissTM~\cite{swisstm} is also a word-STM, but increases performance by tracking memory in 4-word groups, resulting in less overhead than tracking individual memory words. There have also been object-based STMs which track objects instead of memory words, but incur extra cost by shadow copying any objects written to within the transaction. 

Open nesting~\cite{opennesting}, elastic transactions~\cite{elastic}, transactional collection classes~\cite{tcc}, early release~\cite{earlyrelease}, and SpecTM~\cite{spectm} are techniques for implementing transactions that, like STO, speed up transactional performance by reducing the bookkeeping costs and the number of false conflicts. However, these techniques expose non-transactional APIs to the programmer, which requires the programmer to be an expert in the system in order to implement transactions. This complicates, rather than simplifies, concurrent programming. For example, transactional collection classes remove unnecessary memory conflicts in data structures by wrapping the datatype with semantic locks; this requires designing multi-level, open-nested transactions, and presents a much more complicated framework than an STM such as STO that allows the datatype to be designed specifically to support transactions.

STO is one of several STM systems that use abstraction to improve STM performance~\cite{predication}\cite{autolock}\cite{optboost}\cite{boost}. These systems expose a transactional API to programmers in the form of concurrent, transactional data structures that are written on top of an STM. However, systems other than STO build their data structures on top of traditional word-STMs, whereas STO builds data structures on top of an abstract STM which tracks abstract items defined by each data structure. This lets STO improve performance beyond that of previous systems. 

Our work focuses on datatypes and how they perform within transactional settings. We now take a closer look at STO and other abstract STM systems that expose an API in the form of transactional data structures allowing programmers to use transactions.

\section{Abstraction-based STMs}

The STO system tracks abstract operations of a transactional abstract datatype. STO consists of a core system that implements a transactional, optimistic commit-time protocol, and an extensible library of transactional datatypes built on top of this core. While programmers can use the many datatypes already implemented (ranging from queues to red-black trees), STO provides a transactional framework that allows programmers to easily add transactional support to other datatypes based on their particular semantics. STO allows datatypes to add datatype-specific \emph{items} to their read or write sets, thereby reducing bookkeeping costs by 
exploiting the semantics of the particular datatype. Furthermore, datatypes can choose to use a variety of different concurrency control algorithms. For example, STO's transactional hashmap defines abstract items for each bucket, which are invalidated only when the bucket size changes. This means that transactions conflict only when modifying or reading the same bucket, allowing scalable access to the data structure. In addition, at most one item is added to the read/write set per operation, which can be orders of magnitude fewer than the number of items tracked by a word- or object-based STM. STO allows datatypes to define their own strategies for transaction execution: a datatype can insert elements during transaction execution (eagerly) or wait until commit time to insert (lazily). The specifics of the commit protocol are implemented as datatype callbacks. This allows a datatype to use pessimistic strategies for certain operations (e.g., by needing to acquire a lock) while using optimistic strategies for other operations. STO is therefore a flexible hybrid of optimistic/pessimistic and eager/lazy versioning strategies.

Boosting~\cite{boost} is a method to convert highly-concurrent (non-transactional) data structures into transactional data structures. Like STO, boosting determines conflicts between transactions by relying on particular data structure semantics. Instead of allowing each datatype to define read- and write-set items, boosting maps a data structure's operations to abstract locks. If two operations do not commute---i.e., swapping the order of their invocations affects the final state of the data structure or the responses returned by the operations---then the abstract locks for the operations will conflict: for one of the operations to be performed, both of the abstract locks must be acquired. Thus, the granularity of synchronization of the original linearizable data structure is only achievable in a boosted data structure for commutative operations. Because a transaction may abort, boosting practices undo logging and requires that each operation has an inverse. These constraints prevent boosting from applying to all data structures, but make it particularly useful for ones like sets. Unlike boosting, STO allows us to work with data structures whose operations have no inverse. Boosting is an inherently pessimistic strategy, using eager versioning, whereas STO allows for a hybrid approach that can improve performance.

Optimistic boosting~\cite{optboost} is a technique meant to improve the performance of boosting. It proposes an optimistic approach, in which acquisition of the abstract locks is delayed until commit time. During execution, semantic items are added to the read and writes sets of the transaction and validated at commit time; if all reads are valid, then the appropriate abstract locks are acquired and modifications applied to the data structure. Because execution is optimistic and abstract locks are not eagerly acquired at the higher, ``boosted'' level, the underlying concurrent data structure can be lazy and not execute operations until commit. This adds support for operations that may not have an inverse. 
However, optimistic boosting has not been shown to be effective in practice. STO provides a more flexible hybrid transactional framework and achieves larger performance gains than has been demonstrated by optimistic boosting.

Automated locking~\cite{autolock} takes a similar approach to boosting by pessimistically acquiring abstract locks corresponding to each operation. It differs from boosting because it also takes a datatype-specific commutativity specification of conditions in which operations commute. The commutativity specifications of an abstract datatype is compiled into a symbolic set (called a ``locking mode'') that is used to prevent conflicting operations from being run concurrently. If two operations commute, the ``locking mode'' allows them to run concurrently.
This approach optimizes and automates the creation of abstract locks for an abstract data structure. A similar approach may help STO data structures choose which abstract read/write items to track during a transaction to avoid false conflicts.

Predication~\cite{predication} is a technique that maps operations to ``predicate words'' contained in a shared-memory table managed by the STM. Each predicate word represents a property of the data structure. For example, looking up an element $e$ in a set would be associated with an \texttt{in\_set?($e$)} predicate word, and the STM would add reads or writes of the predicate word when $e$ is removed or added to the set. 
Unlike boosting, transactional predication achieves semantic conflict detection without keeping an undo log. However, predication must insert predicate words into the table for absent as well as present lookups, causing a garbage collecting problem that STO and other systems do not face. Transactional predication also focuses on making transactional data structures perform equally as well as highly-concurrent data structures in non-transactional settings. This is orthogonal to our work here and can be a future line of optimization for STO data structures.

The Transactional Data Structures Libraries (TDSL)~\cite{tdsl}, like STO, produces collections of transactional data structures for programmers to easily use. Similar to STO, TDSL allows for both pessimistic/optimistic and eager/lazy strategies and customizes strategies for each individual data structure. This allows for optimizations that rely on the data structure's semantics. Unlike STO, data structures in TDSL are also optimized for single-operation transactions (which we see as an orthogonal line of work). While TDSL contains only a queue and skiplist, STO has implementations of many other data structures in transactional settings, including a queue, hashmap, list, priority queue, and red-black tree. Our work in this thesis draws upon some of the algorithm designs for the queue implemented in TDSL.

This thesis investigates the integration of several highly-concurrent (non-transactional) data structures with STO. In particular, we test and modify different concurrent queue and concurrent hashmap algorithms, which we describe in more detail in Chapters~\ref{queue} and~\ref{hashmap}. Similar work has been done with lazy sets~\cite{lazyset}, in which transactional support is added to a lazy concurrent set. We extend their work to other data structures and investigate further how to achieve performance close to highly concurrent (non-transactional) data structures.

\subsection{Commutativity in Transactional Data Structures}

STO, along with the above methods of integrating concurrent abstract datatypes with STM, builds upon the ideas introduced by Weihl in the late 1980s~\cite{weihl}. Weihl defines optimal local atomicity properties of datatypes that allow for transactions to be serialized, therefore providing an upper bound on the amount of concurrency achievable in transactional datatypes. These local properties are derived from algebraic properties of the data structure, such as commutativity of particular operations. Weihl also demonstrates an inherent relation between commutativity-based concurrency control and transactional recovery algorithms. Unlike our work with STO, however, Weihl does not focus on the implementation of these properties.

Schwarz and Spector~\cite{schwarz} introduce a theory to order concurrent transactions based on the semantics of shared abstract datatypes and dependencies of different datatype operations. For each operation, the programmer specifies the operation's preconditions, postconditions, and invariants. To describe interactions between operations in a transaction, the programmer additionally specifies an interleaving specification. This defines dependency relations between datatype operations. From this set of dependency relations, Schwarz and Spector can define the limits of concurrency for the datatype by drawing upon results from Korth~\cite{korth} that show when two operations commute (have no inter-dependencies), the order in which the operations are ordered do not affect serializability. We discuss this work further in Chapter \ref{commutativity}. 

Schwarz and Spector also demonstrate that increased concurrency can be achieved by weakening the serializability of transactions: once the semantics of a datatype have been taken into account, the remaining constraints on concurrency come from enforcing transactional properties~\cite{kung}. Weaker ordering properties allow a datatype to fully exploit its operation semantics. They exemplify this with a WQueue design: a higher-concurrency queue with modified semantics that preserves a weaker ordering property than serializability. However, their paper focuses on the theoretical result instead of the implementation and do not concern themselves with explicit concurrent algorithms for the queue. Our work in Chapter \ref{commutativity} with the weakly-transactional queue builds off this idea, and we provide experimental evidence that operation semantics can be usefully exploited only by providing weaker transactional guarantees.

Badrinath and Ramamritham~\cite{badrinath} investigate operation commutativity to define recoverability, a weaker notion that commutativity that can achieve enhanced concurrency. Their key observation is that recoverability, unlike commutativity, takes into account the \emph{removal} of operations from the execution history. If an operation is recoverable with respect to another transaction's operation, then the operation can be eagerly applied to the data structure without the other transaction's operation aborting or committing. This forces an ordering at commit time, but has the nice property that at least one of the transactions will be able to commit. This prevents issues such as cascading aborts, in which one transaction's abort causes the abort of another transaction, which can cause the abort of a third transaction (and so on).
Commutativity has also played a role in network consistency algorithms and conflict-free replicated data types (CDRTs)~\cite{CRDT}, as well as in the database community (where the idea of transactions originated and has been heavily researched).

Kulkarni, et.al.~\cite{galois} define the notion of a commutativity lattice (predicates between pairs of methods) to reason about commutativity in a data structure. The Galois system, upon which this idea is tested, provides a framework in which the programmer defines a commutativity lattice for individual data structures and, by exploiting commutativity, improves the performance of irregular parallel applications. Galois, however, focuses on constructing commutativity checkers instead of serializing transactions.

Finally, commutativity work has played a large part in optimizing distributed transactions. Mu, et.al.~\cite{distributed} introduce a system, ROCOCO, that first distributes pieces of concurrent transactions across multiple servers. These servers then determine dependencies between their pieces of concurrent transactions and delay all transaction execution until commit time. When a transaction commits, its corresponding dependency information is sent to all servers via a coordinator, allowing the servers to re-order conflicting pieces of the transaction and execute them in a serializable order. This reduces aborts and unnecessary conflicts.
