\chapter{Introduction}
\section{STM and STO}
Parallelism is increasingly critical for performance in computer software systems, but parallel programming remains enormously challenging to get right. Low-level mechanisms for coordinating threads, such as lock-based strategies, are fragile and error-prone. To address this problem, researchers have developed programming tools and methodologies for managing parallelism. Prominent among these is software transactional memory (STM), which allows programmers to write concurrent code using sequential programming paradigms. By using STM, programmers reason about concurrent operations on shared memory through transactions---atomic groups of operations---instead of single operations. 

Unfortunately, STM often results in high overhead and is rarely considered practical. To provide transactional guarantees, an STM system tracks the different memory words accessed within a transaction and ensures that these words are not touched by a separate, simultaneous transaction. Traditional word-based STM tracks all words of memory read or written in a transaction, incurring enormous overhead~\cite{cascaval}. To address this problem, a research team at Harvard has produced a novel type of STM, STO (Software Transactional Objects), that greatly improves upon the performance of traditional STM~\cite{sto}. The system's implementation works at a higher level than most previously developed systems: data structure operations, rather than individual memory words. For example, a word-STM tracks every word accessed in the path from the root during a binary search tree lookup, which leads to significant overhead from transactional bookkeeping and the introduction of unnecessary conflicts: the transaction will abort if there is a concurrent update to the path, even though the result of the lookup may be unaffected. STO allows datatypes to define datatype-specific abstract objects to track instead of memory words. Thus, during a binary search tree lookup, STO will track only the abstract object corresponding to the parent of the searched-for node, and use only this object to detect a conflict in which the node is modified, removed, or inserted. Compared to a word-STM, STO achieves both a reduced number of false conflicts and hundreds of times fewer objects to track.

\section{Motivation}
The focus of this work is to make STO as fast as the fastest concurrent programming patterns available, and when this is impossible, to characterize precisely why. Although STO outperforms traditional STM, STO’s performance still falls far below that of other concurrent programming paradigms such as Flat Combining~\cite{flatcombining}. STO’s library of transactional datatypes---datatypes exposing transactional operations---allows programmers to add transactional memory to their programs. Thus, STO programs are only as fast as STO's datatypes. A natural focus for our research is then transactional data structure algorithms: by defining the limits and potentials of these algorithms, we learn how to maximize the performance of the STO system.

\section{Overview}
While our research's scope includes all the different datatypes supported by STO, this thesis focuses on a few core data structures: queues and hashmaps. We first implemented the initial version of these STO datatypes and, more broadly, developed design techniques for transactional data structures. These techniques defined general patterns for designing transactional algorithms, such as how to handle reads and writes of the same object within the same transaction. These patterns, however, do not maximize scalability or performance.

To address our goal of performance maximization, we shifted our focus to analyze and compare the performance of existing STO data structures against implementations of highly-concurrent, non-transactional data structure algorithms from recent research. These concurrent data structure algorithms strive to maximize scalability and performance without the concern for transactional correctness: they ensure that single operations can execute concurrently in multiple threads but provide no guarantees that a thread can execute a sequence of operations (a transaction) without interruption from another thread (atomically).
Thus, the performance of the fastest concurrent datatypes available acts as an upper bound for the performance transactional STO data structures may reasonably hope to achieve. Our benchmarks highlight which concurrent, non-transactional data structures are the highest-performing, and which parts of the transactional data structure algorithms are bottlenecks and areas for improvement. 

We first hypothesize that combining concurrent, non-transactional programming patterns with our transactional design patterns will produce transactional datatypes that greatly outperform our previous implementations. To evaluate this hypothesis, we take the fastest concurrent datatype algorithms and implement them within the STO framework. We discover that the algorithmic changes necessary to move highly-parallel data structures into STO results in a significant decrease in their performance; at times, they even underperform our initial STO data structures that use naive algorithms for concurrency.
Furthermore, our experience combining highly-concurrent, non-transactional algorithms with transactional algorithms leads to the conclusion that reasoning about scalability in transactional datatypes is inherently different than reasoning about scalability in traditional concurrent datatypes. In particular, reasoning about invariants regarding datatype state is essential to handle transactions: a transactional algorithm must maintain state across operations within a transaction. We formalize this argument as a commutativity argument, drawing on previous work regarding commutativity in transactional objects~\cite{weihl}.

The stateful nature of transactions (and therefore reduced operation commutativity) leads us to revise our hypothesis. From our experimental results and experiences creating transactional data structures, it seems clear that the reasoning that allows a highly-concurrent data structure to achieve fast performance and scalability is intrinsically different from the reasoning required to create transactional, highly performant data structures. 
We claim that certain highly-concurrent algorithms for datatypes may be intrinsically non-transactional because the optimizations taken by these algorithms to achieve their high performances are incompatible with providing transactional guarantees. In other words, if the synchronization technique of a concurrent data structure interferes with the techniques required to support transactions, the data structure is unlikely to perform well in a transactional setting. Conversely, if the synchronization technique is mostly or completely independent of STO, meaning that transactional support can be added without modifying the synchronization algorithm itself, then we may still retain the benefits of the data structure's algorithmic optimizations in a transactional setting.

To evaluate our claim, we take highly-concurrent algorithms that underperformed when integrated with STO and investigate how these algorithms act when the transactional interface is modified. 
With an alternative interface, we achieve a better separation between the logic needed for transactional support and the logic of the concurrency algorithm: this allows us to adapt concurrent datatypes algorithms for transactional settings while retaining their high performance.
Redefining the transactional specification demonstrates the trade-offs between the guarantees STO datatypes can provide and their performance: for example, by preventing a thread from seeing its own writes (a violation of strong transactional guarantees), performance of the datatype more than doubles.
We also note that highly-concurrent algorithms whose core optimizations rely on reasoning independent from the reasoning used in our transactional techniques still provide performance benefits in the original transactional setting.

\subsection{Roadmap}
This work is divided into the following parts: 
\begin{itemize}
    \item Background information on transactions and transactional memory, and related work on transactional data structure algorithms and their scalability (Chapter~\ref{related_work})
    \item An evaluation of different transactional and concurrent algorithms for queue and hashmap data structures (Chapters~\ref{queue} and~\ref{hashmap})
    \item An explanation based on operation commutativity and invalid histories for why concurrent algorithms for queues fail to retain their performance benefits when moved into the STO transactional framework (Chapter~\ref{queue})
    \item A demonstration of how the performance loss of queue concurrent algorithms can be elided through changing the transactional specification (Chapter~\ref{commutativity})
    \item A demonstration of and explanation why highly-concurrent hashmap algorithms do, in fact, retain performance benefits in a transactional setting (Chapter~\ref{hashmap})
\end{itemize}
