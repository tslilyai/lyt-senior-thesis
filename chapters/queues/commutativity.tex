\chapter{Commutativity in Transactional and Weakly Transactional Queue Specifications}
\label{commutativity}

We first describe the commutativity of queue operations in highly-concurrent, non-transactional settings and compare it to the commutativity of queue operations in a transactional setting. We then argue that the flat combining technique, while perhaps near-optimal for a highly-concurrent data structure, is no better for performance than a naive synchronization technique in a transactional data structure. This is because the flat combining algorithm's high performance comes from exploiting the greater operation commutativity present in a non-transactional setting. The flat combining algorithm's optimizations require heavy modifications to support transactions, leading to significant performance loss. 

To demonstrate this point, we propose a transactional specification that allows for greater operation commutativity, with the expectation that the flat combining technique can achieve better performance under this specification. Our experimental results illustrate that the commutativity of operations in this new setting is essential for the flat combining technique to be effective.

\section{Commutativity Terminology and Results}
We introduce some basic terminology (as defined by Schwarz~\cite{schwarz} and Weihl~\cite{weihl}) that will occur in our discussion.

\subsection{Histories}
\begin{defn}
    A \emph{history} is a sequence of \texttt{(transaction, operation, result)} tuples that represent an interleaving of operations of all committed transactions. Knowledge of both the history and initial conditions of a data structure leads to complete knowledge of the (high-level) end state of the structure.

\begin{eg}
    \singlespacing   

    \begin{lstlisting}

    // Q.size() == 0 
    (T2, Q.push(a), ())
    (T1, Q.pop(), true)
    (T2, Q.push(a), ())
    (T1, Q.pop(), true)
    // Final State: Q.size() == 0 
    \end{lstlisting}
    \doublespacing
\end{eg}

\end{defn}

\begin{defn}
    A history $H'$ is \emph{consistent} with $H$ if:
    \begin{enumerate}
        \item $H'$ contains the same tuples as $H$: the same transactions were executed with the same return values for all operations within the transactions.
        \item The order of a single thread's calls in $H'$ remains consistent with the thread's order of calls in $H$.
    \end{enumerate}
\end{defn}

\begin{defn}
    A history $H$ is \emph{serial} if all tuples are ordered as if all transactions were executed in a serial order.
\end{defn}
\begin{defn}
    A history $H$ is \emph{serializable} if there exists a serial history $H'$ s.t. $H'$ is consistent with $H$.

\end{defn}

\begin{eg}
$H$ is a serializable history whose corresponding serial execution is $H'$. $H''$ represents a serial history but is inconsistent with $H$.
\begin{figure}[H]
\singlespacing   
   \begin{tabular}{c|c|c}
H & H' & H''\\
\hline
\begin{lstlisting}
// Q.size() == 0 
(T2, Q.push(a), ())
(T1, Q.pop(), true)
(T2, Q.push(a), ())
(T1, Q.pop(), true)
\end{lstlisting} & 
\begin{lstlisting}
// Q.size() == 0 
(T2, Q.push(a), ())
(T2, Q.push(a), ())
(T1, Q.pop(), true)
(T1, Q.pop(), true)
\end{lstlisting} &
\begin{lstlisting}
// Q.size() == 0 
(T1, Q.pop(), false)
(T1, Q.pop(), false)
(T2, Q.push(a), ())
(T2, Q.push(a), ()) 
\end{lstlisting}
\end{tabular}
\end{figure}
\end{eg}

\begin{defn}
    A history $H$ is \emph{strictly serializable} and therefore \emph{valid} if it is both serializable and all transactions are linearizable. \emph{Linearizability} means that a transaction appears to occur instantaneously between the time of its start and the time of its commit: if transaction $T1$ commits before transaction $T2$, then $T1$ must appear before $T2$ in the serial history~\cite{harristm}.
\end{defn}

\begin{eg}
$H$ is a serializable, but not linearizable history. $H'$ is a valid serial ordering of $H$, but violates the rule that the serial order of transactions corresponds to the real time order of the transactions' commits.
    
\begin{figure}[H]
\singlespacing   
    \begin{tabular}{c|c}
H & H'\\
\hline
\begin{lstlisting}
// Q empty                          
(T1, Q.push(a), ())                
(T1, Q.push(a), ())               
(T1, Q.pop(), true)  // T1 commits
(T2, Q.pop(), false) // T2 commits
\end{lstlisting} & 
\begin{lstlisting}
// Q empty
(T2, Q.pop(), false) // T2 commits
(T1, Q.push(a), ())                       
(T1, Q.push(a), ())
(T1, Q.pop(), false)// T1 commits
\end{lstlisting}
    \end{tabular}
\end{figure}
\end{eg}

\subsection{Dependencies}

Note that all operations can be classified as sets of reads and/or writes (as we do in Table~\ref{table:qrw}). We therefore define dependencies abstractly as reads and writes of particular objects in our definitions.

\begin{defn}
    A \emph{dependency} between transaction $T2$ and transaction $T1$ can be defined as one of the following relations:
    \begin{itemize}
        \item R-R: $T2$ reads an object previously read by $T1$
        \item R-W: $T2$ writes an object previously read by $T1$
        \item W-R: $T2$ reads an object previously written by $T1$
        \item W-W: $T2$ writes an object previously written by $T1$
    \end{itemize}
    Dependencies between two transactions form a dependency graph, where transactions are the vertices and labeled edges indicate different types of dependencies between them.
\end{defn}

\begin{defn}
    Operation $p$ performed by $T1$ \emph{commutes} with operation $q$ of $T2$ when the operations form a \emph{R-R} dependency or no dependency at all~\cite{weihl}.
\end{defn}

\subsection{Commutativity and Serializability Results}

These results are well-known in the literature about transactional data structure scalability; we repeat them here for reference.
A history $H$ is serializable if there are no cycles in \emph{R-W}, \emph{W-R}, or \emph{W-W} dependencies between any two transactions in the dependency graph (see Schwarz's proof~\cite{schwarz}). This implies that when two operations commute, exchanging the order in which they occur in the history does not affect the serializability of the history. From this, it follows that the number of valid histories of a transactional data structure is dependent on the commutativity of its operations.

These results imply that commutativity of a data structure's operations determines the data structure's scalability. In the context of a transactional data structure, a greater number of valid histories leads to lower contention between transactions and greater scalability. 

\section{Commutativity of Concurrent and Transactional Queues}

For generality, we reduce each queue operation to a read or write of a particular semantic object: the \texttt{head}, the \texttt{tail}, or the \texttt{empty?} predicate of the queue. This allows for our reasoning to be applied to push or pop operations that differ from our current specification of pop or push. For example, we can imagine an alternative pop operation that returns \texttt{void} regardless of whether the queue was empty, which would perform no visible reads. We summarize the reads and writes of our pop and push specifications in Table \ref{table:qrw}.

\begin{table}[t]
\centering
\begin{tabular}{c||c|c}
    Operation & Read & Write\\
    \hline
    pop & \texttt{empty?} & \texttt{head}, \texttt{empty?}\\
    push & & \texttt{tail}, \texttt{empty?}\\
\end{tabular}
    \caption{Objects read or written by queue operations}
    \label{table:qrw}
\end{table}

\subsection{Concurrent Non-Transactional Queues}

The guarantees of concurrent, non-transactional queues are \emph{nearly} equivalent to that of singleton transactions. A history of singleton transactions is automatically serializable: the history corresponding to the ordering of operation execution is a serial ordering of transactions. The atomicity of transactions is guaranteed by the correctness properties of the concurrent data structure. However, single operations are not always linearizable: depending on the implementation of the data structure, the effects of a operation $P$ that has ``committed'' (i.e., has returned) may not be visible to an operation that is performed after $P$ returns.

The flat combining technique provides serializable, atomic, \emph{and} linearizable singleton transactions~\cite{flatcombining}. We can therefore reason about the commutativity and therefore scalability of a concurrent, non-transactional flat combining queue using notions of transactional dependencies.

We start by defining the dependency relations between single operations (Table~\ref{table:queuesimpledeps}). A concurrent, non-transactional queue cannot have any cyclical dependencies, since such dependencies necessarily require that there exists a transaction with more than one operation. Therefore, synchronization is only necessary to ensure correctness when two threads attempt to write (\emph{W-W}) or when simultaneously one thread writes and another reads (\emph{W-R} or \emph{R-W}) the same object: the main performance bottleneck is concurrent access synchronization. As we have shown, the flat combining approach works particularly well in a highly-concurrent setting to minimize the overhead of this synchronization cost.

\begin{table}[t]
    \centering
\begin{tabular}{c||c|c|c|c}
    Object & Pop-Pop & Pop-Push & Push-Pop & Push-Push\\
    \hline
    head & W-W & & & \\
    tail & & & & W-W\\
    empty & W$^e$-R & R-W$^e$ & W$^e$-R, W$^e$-W$^e$ & \\
\end{tabular}
    \caption*{$X$-$Y$ = thread 1 performs operation $X$, thread 2 performs operation $Y$.\\$X^e$ = the operation X modifies the empty status of the queue.\\R-R relations are not shown.}
    \caption{Dependencies of pairs of queue operations}
    \label{table:queuesimpledeps}
\end{table}

\subsection{Transactional Queues}

\begin{table}
    \centering
    \begin{tabular}{|l|}
        \hline
\begin{lstlisting}
1) // Q.size() > 1 
   (T1, Q.pop(), true)  
   (T2, Q.pop(), true)       // W-W
   (T1, Q.pop(), true/false) // W-W
\end{lstlisting}
       \\ 
    \hline
\begin{lstlisting}
2) // Q.size() == 1  
   (T1, Q.pop(), true) // Q empty  
   (T2, Q.push(a), ()) // R-W
   (T1, Q.pop(), true) // W-R
    \end{lstlisting}
       \\ 
    \hline
\begin{lstlisting}
3) // Q.size() == 1  
   (T1, Q.pop(), true)  // Q empty  
   (T2, Q.pop(), false) // W-W     
   (T1, Q.push(a), ())  // R-W     
   \end{lstlisting} 
\\
\hline
\begin{lstlisting}
4) // Q.size() >= 0 
   (T1, Q.push(a), ()) 
   (T2, Q.push(a), ()) // W-W
   (T1, Q.push(a), ()) // W-W
\end{lstlisting}
\\
\hline
\begin{lstlisting}
5) // Q.size() == 0 
   (T1, Q.push(a), ())       
   (T2, Q.pop(), true)  // W-R, Q empty
   (T1, Q.pop(), false) // W-W
\end{lstlisting}
\\
    \hline
\end{tabular}
    \caption*{Interleavings that create R-R dependencies or no dependencies are omitted.}
    \caption{Operation interleavings generating dependency cycles.}
    \label{tab:interleavings}
\end{table}

Cyclical dependencies can occur within a fully transactional queue, thus reducing the number of valid histories. The possible interleavings that generate cyclical dependencies are shown in Table~\ref{tab:interleavings}. Nearly all of the interleavings that result in cyclical dependencies, and therefore invalid histories, occur when the queue becomes empty. 
We only see cyclical dependencies in a nonempty queue if both transactions push or pop (a \emph{W-W} dependency).

A transactional queue prevents these invalid interleavings from occurring through delayed push operation execution and a pessimistic or optimistic approach when encountering an empty queue.

To prevent interleavings 4 and 5, all pushes are delayed until commit time. These interleavings can occur only if $T1$'s first push is visible to $T2$ prior to $T1$'s commit. If we delay pushes until commit time, $T2$ will not detect the presence of a pushed item in the queue.

Because pop operations immediately return values that depend on the state of the queue (\texttt{false} if the queue is empty or \texttt{true} if the queue is nonempty), interleavings 1, 2, and 3 cannot be prevented by delaying pop operations until commit time. Instead, we can take one of two approaches. Assume $T1$ is a transaction that has performed a pop.
\begin{enumerate}
    \item Optimistic: Abort $T1$ at commit time if $T2$ has committed an operation that would cause an invalid interleaving.
    \item Pessimistic: Prevent $T2$ from committing any operation until after $T1$ commits or aborts.
\end{enumerate}

The T-QueueO implements the optimistic method: checks of the \texttt{tailversion} and \texttt{headversion} determine at commit time whether the empty status of the queue has been modified by another, already committed transaction. The T-QueueP implements the pessimistic approach, which locks the queue after a pop is performed and only releases the lock if the transaction commits or aborts, therefore preventing any other transaction from committing any operation after the pop.

To support transactions, the flat combining approach must do either approach (1) or (2). Here we argue that the flat combining approach cannot do either without introducing overhead that reduces its performance to below that of the T-QueueO or T-QueueP.

If we take approach (1), a pop cannot be performed at execution time because no locks on the queue are acquired at execution time: other transactions are allowed to commit pops, which may pop an invalid head if this transaction aborts. Thus, in order to determine if a pop should return true or return false, a transactional pop flat combining request requires much more complexity than a non-transactional one: the thread must determine how many elements the queue holds, how many elements the current transaction is intending to pop, and if any other thread intends to pop (in which case the transaction aborts). The transactional push flat combining request is also significantly more complex, as it requires installing all the pushes of the transaction. Additional flat combining calls are necessary to allow a thread to perform checks of the queue's empty status (the \texttt{<EMPTY?>} flat combining call) to determine whether the transaction can commit or must abort, and to actually execute the pops at commit time. Thus, approach (1) requires adding both more flat combining calls and more complexity to the existing flat combining calls.

If we take approach (2), the flat combining approach can either perform a pop at execution time or delay the pop until commit time. If the pop is performed at execution time, then the thread must acquire a global lock on the queue after a pop and hold the lock until commit: this prevents another thread from observing an inconsistent state of the queue. If a pop removes the head of the queue prior to commit and the transaction later aborts, the popped element must be re-attached to the head of the queue. Any thread performing a pop must acquire a global lock to ensure that no other thread can commit a transaction that pops off the incorrect head of the queue (given that elements may be reattached to the head if the transaction aborts). Additional flat combining calls are necessary to acquire or release the global lock. 

We can also imagine a mix of approaches (1) and (2). If a transaction $T1$ executes a pop, we can disallow any pops from other transactions (using the equivalent of a global lock) but allow other transactions containing only pushes to commit prior to $T1$ completing. This approach prevents interleavings 1 and 3, but requires performing a check of the queue's empty status, as in approach (1), if a pop saw the queue in an empty state. This is because another transaction may have committed a push between the time of $T1$'s pop and $T1$'s completion. This mixed approach outperforms both approach (2) and approach (1), and is the approach described as the flat combining algorithm in Chapter~\ref{queue}. 

As previously noted, all possible approaches to prevent interleavings 1, 2, and 3 rely on implementing additional flat combining calls and increasing the complexity of previously existing flat combining calls. In addition, acquisition of a global ``lock'' on the queue for approach (2) prevents the combiner thread from applying \emph{all} of the requests it sees; instead, requests will either return ``abort'' to the calling thread or not be applied, leading to additional time spent spinning or repeating requests. We see through our experiments that these changes to the flat combining algorithm reduce its performance such that it performs worse than a naive synchronization algorithm; furthermore, we claim that these changes are necessary in order to provide transactional guarantees. The original flat combining algorithm exploits the property that any correct history of operations in data structures supporting only singleton transactions (i.e., a concurrent, non-transactional data structure) is valid. The combiner thread is allowed to immediately apply all thread's operation requests in arbitrary order. However, this property that makes flat combining so performant disappears as soon as the algorithm has to deal with invalid histories. In the next section, we demonstrate how ignoring invalid histories leads to a version of flat combining that can outperform our T-QueueO and T-QueueP algorithms: this supports our claim that the flat combining algorithm's performance is heavily dependent on what types of transactional guarantees it must provide.
