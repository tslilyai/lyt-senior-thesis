\chapter{Future Work and Conclusion}

\section{Future Work}
One direction of future work is to specialize our data structures for singleton transactions. As we noted in Chapter~\ref{commutativity}, the commutativity between singleton transactions is equivalent to the commutativity between single operations. Special treatment of singleton transactions allows singletons to avoid the transactional overhead that is necessary to synchronize multi-operation transactions. However, singletons would need to be carefully handled if they interleave with multi-operation transactions. 

This work defines a class of concurrent data structure interfaces that do not suffer a high loss of commutativity in a transactional setting, and proposes that data structures implementing these interfaces will retain their scalability when modified to provide transactional guarantees. This claim can be tested further by implementing and benchmarking other data structures that fall into this class. Building these data structures will hopefully also result in additional transactional STO data structures that perform and scale well.

Another class of data structures interfaces defined by this work are those interfaces that suffer crippling performance and scalability loss in a transactional setting. This class includes, for example, the strong queue interface. For any concurrent data structure that falls into this class, alternative specifications for the data structure, such as the one we proposed for the queue, can be explored. These data structure specifications can be tuned to provide some useful guarantees in a transactional setting beyond that of a simple concurrent data structure, while still achieving high performance. 

\section{Conclusion}
This thesis argues that retaining the performance benefits and scalability of highly-concurrent data structure algorithms within a transactional framework such as STO is contingent upon the amount of commutativity that is lost when transactions must be supported. The amount of commutativity between transactions determines the amount of independence between the synchronization strategy used by the highly-concurrent algorithm, and the transactional bookkeeping and mechanisms that must be added to provide transactional guarantees.

Our investigation into concurrent and transactional queue algorithms demonstrates that there is a large performance gap between our naively-concurrent transactional queues, and the best-performing non-transactional concurrent queue (the flat combining queue). However, the flat combining queue suffers crippling performance loss when moved into STO. This is because the flat combining technique relies on operation commutativity that is prohibited in a transactional setting; the fundamental principle of flat combining is that operations from different threads can be applied in an arbitrary order to the queue, which is no longer true when operations are performed within transactions. In order for flat combining to support transactions, it must be modified in ways that greatly reduce its effectiveness. By exploring an alternative queue specification that allows for greater operation commutativity in a transactional setting, we provide evidence that the effectiveness of the flat combining technique is dependent on operation commutativity.

As an example of the opposite phenomenon, in which a concurrent algorithm retains its performance benefits and scalability in a transactional setting, we look to cuckoo and chaining hashmap algorithms. Our hashmap interface experiences fewer added commutativity constraints in a transactional setting than does our strong queue interface. Consequently, both the transactional cuckoo and transactional chaining hashmaps retain their scalability.
Furthermore, the beneficial properties of cuckoo hashmaps (such as good performance in a small hashmap with several values in each bucket) are present even in a transactional setting. This is because the cuckoo hashing synchronization algorithm can be implemented independently of the modifications necessary to support transactions. In other words, the lack of added commutativity constraints in the transactional setting means that the concurrent cuckoo hashmap algorithm can be made transactional without fundamentally changing its behavior.

Our results provide a way to determine in advance whether a highly-concurrent, non-transactional data structure can be made transactional while still achieving high scalability and performance. By evaluating how much commutativity a particular data structure's interface loses when moving from a non-transactional to a transactional setting, we also evaluate the impact on the data structure's performance and scalability when it is modified to support transactions. 
This general method enables us to explain why and how different transactional data structures achieve the performance that they do. With this method, researchers can focus on data structure designs that have the potential for high performance in transactional settings, and avoid unfortunate surprises from data structures that are destined to underperform.
