\chapter{Future Work and Conclusion}

\section{Future Work}
To further increase performance, our data structures can be specialized for singleton transactions. As we saw in the commutativity discussion of Chapter~\ref{commutativity}, the commutativity between singleton transactions is equivalent to the commutativity between single operations. Special treatment of singleton transactions allows singletons to avoid transactional overhead that is necessary to synchronize multi-operation transactions. However, singletons would need to be carefully handled in the cases in which they interleave with multi-operation transactions. 

We would also like to further test the idea that only those concurrent data structures whose interfaces do not suffer from a high loss of commutativity in a transactional setting will scale well when modified to provide transactional guarantees. Given a concurrent data structure with this property, we hope to construct more transactional STO data structures that perform and scale well.
For concurrent data structures whose performance is crippled in a transactional setting (such as the FIFO queue or priority queue), alternative interfaces for these data structures, such as the one we proposed for the queue, can be explored. These data structure specifications can be tuned to provide some useful guarantees in a transactional setting beyond that of a simple concurrent data structure, while still achieving high performance. 

\section{Conclusion}
This thesis argues that retaining the performance benefits and scalability of highly-concurrent data structure algorithms within a transactional framework such as STO is contingent upon the amount of commutativity that is lost when transactions must be supported. The amount of commutativity between transactions determines the amount of independence between the synchronization strategy used by the highly-concurrent algorithm, and the transactional bookkeeping and mechanisms that must be added to synchronize transactions.

Our investigation into concurrent and transactional queue algorithms demonstrates that there is a large performance gap between our naively-concurrent transactional queues, and the best-performing non-transactional, concurrent queue (the flat combining queue). However, the flat combining queue suffers crippling performance loss when moved into STO. This is because the flat combining technique relies on operation commutativity that is disallowed in a transactional setting, and therefore the modifications to flat combining required to support transactions reduce the effectiveness of the flat combining technique. Through exploration of an alternative queue specification that allows for greater operation commutativity in a transactional setting, we provide evidence that the effectiveness of the flat combining technique is dependent on operation commutativity.

As an example of the opposite phenomenon, in which a concurrent algorithm retains its performance benefits and scalability, we look to cuckoo and chaining hashmap algorithms. Our hashmap interface experiences fewer added commutativity constraints in a transactional setting than does our strong queue interface. Consequently, both the transactional cuckoo and transactional chaining hashmaps scale.
Furthermore, the beneficial properties of cuckoo hashmaps (such as good performance in a small hashmap with several values in each bucket) are present even in a transactional setting. This is because the cuckoo hashing synchronization algorithm can be implemented independently of the transactional bookkeeping to support transactions: the lack of added commutativity constraints in the transactional setting means that the concurrent cuckoo hashmap algorithm can be made transactional without fundamentally changing its behavior.

Our results provide a way to determine in advance whether a highly-concurrent, non-transactional data structure can be integrated with STO while still achieving high scalability and performance. By evaluating how much commutativity a particular data structure's interface loses when moving from a non-transactional to a transactional setting, we also evaluate how much impact the modifications necessary to support transactions will have on performance and scalability. 
This general method, which enables us to explain why and how different transactional data structures achieve the performance that they do, can help researchers focus on data structure designs that have the potential for high performance in transactional settings, and avoid those designs that are bound to underperform.
