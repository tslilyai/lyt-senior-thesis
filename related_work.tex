\section{Background and Related Work}

\subsection{Transactions}
A transaction, or a group of operations, allows a programmer to more simply reason about concurrent access to shared state. The concept developed first in database theory and expanded to other domains with the development of hardware transactional memory in 1986 and later software transactional memory (STM) in 1995. Transactions restrict how threads of execution can interact by providing certain guarantees: transactions are \emph{serializable}, which means that one can find an ordering of committed transactions that satisfies the observed history of the execution. Operations within one transaction are never interleaved with operations in another transaction. Transactions are also \emph{atomic}: if a transaction commits, all changes made by the transaction are instantly visible to other threads; if a transaction aborts, no other thread sees any of the changes made by the transaction. Finally, transactions are \emph{linearizable}\cite{linearizability}: all transactions performed at a later clock time than a committed transaction observe the changes made by the committed transaction. This allows programmers to easily determine the order and effects of transactions.

An optimistic transactional system\cite{harristm} enforces transactional guarantees in the following way: first, during a transaction's execution, the system tracks any intended changes to be made during the transaction in the transaction's \emph{write set} and any state observed during the transaction in the transaction's \emph{read set}. When the transaction attempts to commit, the system checks whether the observed values in the read set are still valid. If so, the system performs the changes in the transaction's write set and the transaction is marked committed. Else the transaction aborts and the system ensures that the transaction leaves no visible effects.

A pessimistic transactional system instruments every read and write such that if a conflict is detected when the transaction is executing a read or write prior to commit time, then either the transaction aborts or is stalled until the conflicting transaction commits. Software transactional memory systems---the transactional systems concerned in this work---vary from fully pessimistic to fully optimistic.

Transactional systems can also practice eager versioning or lazy versioning. A system is eager when it updates shared memory during the transaction's execution. It maintains an ``undo'' log of changes should the transaction abort and the changes need to be undone. A system is lazy when no changes are done until the transaction commits: all intended changes during execution are buffered by the system and applied at commit time. Again, systems can range from fully eager to fully lazy, with most practicing a mix of the two techniques.

\subsection{Transactional Memory}
Transactional memory\cite{harristm}\cite{herlihytm} guarantees transactional properties in shared memory. TM can be implemented in both hardware and software. Although hardware transactional memory (HTM) naturally outperforms STMs, but a purely hardware TM has several inherent limitations. HTM will fail when the working sets of the transactions exceed hardware capacities (for example, the size of the buffer cache used to track read/writes of the transaction). In addition, HTM lacks flexibility (granularity of reads/writes is at the word level) and fails to be truly composable\cite{htm}. Nevertheless, with the increasing support for HTM in computer hardware, many STMs, including STO, look to improve performance by integrating STM with HTM.

The STO system\cite{sto} used by this work is one of several developed software transactional memory (STM) systems. For simplicity, we can generalize STMs into three groups: word/object-based STMs, which track individual memory words or objects touched during a transaction, TM with non-transactional APIs for performance gains, and abstraction-based STMs which track items based on abstract datatypes.

TL2\cite{tl2} and LarkTM\cite{larktm} are highly-optimized word-STMs that track memory words touched during a transaction. SwissTM\cite{swisstm} is also a word-STM, but increases performance by tracking memory in 4-word groups, resulting in less overhead than tracking individual memory words. There have also been object-based STMs which track objects instead of memory words, but incur extra cost by shadow copying any objects written to within the transaction. 

There has been significant research in non-transactional APIs, such as open nesting\cite{opennesting}, elastic transactions\cite{elastic}, transactional collection classes\cite{tcc}, early release\cite{earlyrelease}, and SpecTM\cite{spectm}). These allow for programmers to reduce bookkeeping costs and false conflicts between transactions, but also require programmers to be experts in the system to implement such optimizations. This complicates, rather than simplifies, concurrent programming. For example, transactional collection classes remove unnecessary memory conflicts in data structures which are due to choice of implementation instead of semantic necessity; however, this requires designing multi-level, open-nested transactions, a much more complicated framework than an STM such as STO that allows data structures to be designed specifically to support transactions.

STO is one of several STM systems that use abstraction to improve TM performance\cite{boost}\cite{optboost}\cite{autolock}\cite{predication}. These systems expose a transactional API to programmers in the form of concurrent data structures; these data structures are written on top of an STM. However, these systems build their data structures on top of traditional word-STMs, whereas STO builds data structures on top of an abstract STM which tracks abstract items defined by each data structure. This lets STO improve performance beyond that of previous systems. 

Our work focuses on datatypes and how they perform within transactional settings. We now take a closer look at STO and other abstract STM systems that expose an API in the form of transactional data structures that allow programmers to work with transactions.

\subsection{Abstraction-based STMs}

The STM used in this work, STO\cite{sto}, is an STM that tracks abstract operations of an abstract, transactional datatype. STO is comprised of a core system that implements a transactional, optimistic commit-time protocol and an extensible library of transactional datatypes built on top of this core. While programmers can use the many datatypes already implemented (ranging from queues to red-black trees), STO provides a transactional framework that allows further datatypes to easily add transactional support based on its particular semantics. STO allows datatypes to add datatype-specific \emph{items} to their read or write sets to reduce bookkeeping costs and use a variety of different concurrency control algorithms. For example, STO's transactional hashmap defines abstract items for each bucket, which are invalidated only when the bucket size changes. This means that transactions conflict only when modifying or reading the same bucket, which allows scalable access to the data structure. In addition, at most one item is added to the read/write set per operation, which can be orders of magnitude fewer than the number of items tracked by a word- or object-based STM. STO allows datatypes to define their own strategies for transaction execution: a datatype can insert elements during transaction execution (eagerly) or wait until commit time. STO delegates the specifics of the commit protocol to datatype callbacks. This allows a datatype to use pessimistic strategies for certain operations (e.g. by needing to acquire a lock) while using optimistic strategies for other operations. STO is therefore a flexible hybrid of optimistic/pessimistic and eager/lazy versioning strategies.

Boosting\cite{boost} is a method to convert high-concurrency (non-transactional) data structures into transactional data structures. Like STO, boosting determines conflicts between transactions by relying on particular data structure semantics; however, boosting determines conflicts by mapping a data structure's operations to abstract locks. If two operations do not commute---i.e. swapping the order of their invocations affects the final state of the data structure or the responses returned by the operations---then the abstract locks for the operations will conflict. For one of the operations to be performed, both of the abstract locks must be acquired. Thus, the granularity of synchronization of the original linearizable data structure is only achievable in a boosted data structure for commutative operations. Because a transaction may abort, boosting practices undo logging and requires that each operation has an inverse. These constraints limit the applications of boosting to all data structures, but make it particularly useful for ones like sets. Unlike boosting, STO allows us to work with data structures whose operations have no inverse. Boosting is an inherently pessimistic strategy practicing eager versioning, whereas STO allows for a hybrid approach that can improve performance.

Optimistic boosting\cite{optboost} is a technique meant to improve the performance of boosting by proposing an optimistic methodology in acquisition of the abstract locks is delayed until commit time. Instead, semantic items are added to the read and writes sets of the transaction and validated at commit time; if all reads are valid, then the appropriate abstract locks are acquired and modifications applied to the data structure. Because execution is optimistic and abstract locks are not eagerly acquired at the higher, ``boosted'' level, the underlying concurrent data structure can be lazy. This adds support for operations that may not have an inverse. Again, STO provides a more hybrid transactional framework and achieves larger performance gains over boosting than optimistic boosting, which has not been shown to be effective in practice. 

Automated locking\cite{autolock} takes a similar approach to boosting by pessimistically acquiring abstract locks corresponding to each operation. It differs from boosting because it also takes a commutativity specification (of conditions in which operations commute) that allows an abstract datatype to compiles into symbolic sets (locking modes) that are used to lock operations. These locking modes allow two commutative operations to run concurrently. Thus, it is able to optimize and automate the creation of optimize abstract locks for an abstract data structure. A similar approach to automated locking may help STO data structures choose which abstract read/write items to track during a transaction to avoid false conflicts.

Predication\cite{predication} is a technique that maps element to predicates and uses an STM to manage access to the predicate. For example, an element in a set would be associated with an \texttt{in\_set?}\ predicate and the STM would manage reads and writes of the predicate when elements are removed/added to the set. Unlike boosting, transactional predication achieves semantic conflict detection without keeping an undo log. However, predication must create predications for absent as well as present elements, causing a garbage collecting problem that STO and other systems do not face. Transactional predication also focuses on making transactional data structures perform equally as well as high-concurrency data structures in non-transactional settings. This is orthogonal to our work here and can be a future line of optimization for STO data structures.

The Transactional Data Structures Libraries\cite{tdsl}, like STO, aims to produce libraries of transactional data structures. Similar to STO, TDSL allows for both pessimistic/optimistic and eager/lazy strategies and customizes strategies for each individual data structure. This allows for optimizations that rely on the data structure's semantics. Unlike STO, data structures in TDSL are also optimized for single-operation transactions (which we see as an orthogonal line of work). While the example TDSL contains only a queue and skiplist, STO has implementations of many other data structures in transactional settings, including a queue, hashmap, list, priority queue, and red-black tree. Our work in this thesis draws upon some of the algorithm designs for the queue implemented in TDSL.

\lyt{not sure where to put this}
This thesis investigates the integration of several highly-concurrent (non-transactional) data structures with STO. In particular, we test and modify different concurrent queue \cite{queue1}\cite{queue2}\cite{queue3}\cite{queue4}\cite{queue5}\cite{flatcombining} and concurrent hashmap\cite{hm1}\cite{hm2}\cite{hm3}\cite{chm} algorithms, which we describe in more detail in sections \ref{Queue} and \ref{HashMap}. Similar work has been done with lazy sets\cite{lazyset}; their work adds transactional support to a lazy concurrent set. We extend their work to other data structures and investigate further into how to achieve performance close to highly concurrent (non-transactional) data structure.

\subsubsection{Commutativity in Transactional Data Structures}

STO, along with the above methods of integrating concurrent abstract datatypes with STM, builds upon the ideas introduced by Weihl in the late 1980s\cite{weihl}. Weihl defines optimal local atomicity properties of datatypes that allow for transactions to be serialized, therefore providing an upper bound on the amount of concurrency achievable in transactional datatypes. These local properties are derived from algebraic properties of the data structure, such as commutativity of particular operations. Weihl also demonstrates an inherent relation between commutativity-based concurrency control and transactional recovery algorithms. Unlike our work with STO, however, Weihl does not focus on the implementation of these properties.

Schwarz and Spector\cite{schwarz} introduce a theory to order concurrent transactions based on the semantics of shared abstract datatypes and dependencies of different datatype operations. For each operation, the programmer specifies the operation's preconditions, postconditions, and invariants. To describe interactions between operations in a transaction, the programmer additionally specifies an interleaving specification. This defines dependency relations between datatype operations. From this set of dependency relations, Schwarz and Spector can define the limits of concurrency for the datatype by drawing upon results from Korth\cite{korth} that show when two operations commute (have no inter-dependencies), the order in which the operations are ordered do not affect serializability. 

Schwarz and Spector also demonstrate that increased concurrency can be achieved by weakening the serializability of transactions: once the semantics of a datatype have been taken into account, the remaining constraints on concurrency come from enforcing transactional properties\cite{kung}. Weaker ordering properties allows a datatype fully exploit its operation semantics. They exemplify this with a WQueue design: a higher-concurrency queue with modified semantics that preserves a weaker ordering property than serializability. However, their paper focuses on the theoretical result instead of the implementation and do not concern themselves with explicit concurrency algorithms for the queue. Our work in Chapter \ref{WQueue} with the weakly-transactional queue builds off this idea, and we provide experimental evidence that operation semantics can be usefuly exploited only by providing weaker transactional guarantees.

Badrinath and Ramamritham\cite{badrinath} also investigate operation commutativity to define recoverability, a weaker notion that commutativity that can achieve enhanced concurrency. Their key observation is that recoverability, unlike commutativity, takes into account the \emph{removal} of operations from the execution history. If an operation is recoverable with respect to another transaction's operation, then the operation can be eagerly applied to the data structure without the other transaction's operation aborting or committing. This forces an ordering at commit time, but prevents issues such as cascading aborts: at least one of the transactions will be able to commit. Commutativity has also played a role in network consistency algorithms and CDRTs\cite{CRDT}, as well as in the database community (where the idea of transactions originated and has been heavily researched).

Kulkarni, et.al.\cite{galois} define the notion of a commutativity lattice (predicates between pairs of methods) to reason about commutativity in a data structure. The Galois system upon which this idea is tested provides a framework in which the programmer defines a commutativity lattice for individual data structures, and, by exploiting commutativity, improves the performance of irregular parallel applications. Galois, however, focuses on constructing commutativity checkers instead of serializing transactions.

Finally, commutativity work has played a large part in optimizing distributed transactions. Mu, et.al.\cite{distributed} introduce a system, ROCOCO, that first distributes pieces of concurrent transactions across multiple servers. These servers then determines dependencies between their pieces of concurrent transactions and delay all transaction execution until commit time. When a transaction commits, its corresponding dependency information is sent to all servers via a coordinator, allowing the servers to re-order conflicting pieces of the transaction and execute them in a serializable order. This reduces aborts and unnecessary conflicts.
