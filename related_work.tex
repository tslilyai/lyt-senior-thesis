\section{Background and Related Work}

\subsection{Transactional Memory}
The concept of a transaction, or a group of operations, allows a programmer to reason about a transaction as the smallest unit of work. The concept developed first in database theory, then evolved with the development of hardware transactional memory in 1986, and later in 1995 emerged in software transactional memory (STM). Transactional memory\cite{harristm}\cite{herlihytm} simplifies reasoning about parallel executions on shared memory by restricting how threads of execution can interact. Transactions are \emph{serializable}, which means that one can find an ordering of committed transactions that satisfies the observed history of the execution: operations within one transaction are never interleaved with operations in another transaction. Transactions are also \emph{atomic}: if a transaction commits, all changes made by the transaction are instantly visible to other threads; if a transaction aborts, no other thread sees any of the changes made by the transaction. Finally, transactions are \emph{linearizable}\cite{linearizability}: all transactions performed at a later clock time than a committed transaction observe the changes made by the committed transaction. This allows programmers to easily determine the order and effects of transactions.

Transactional memory systems enforce transactional guarantees in the following way: first, during a transaction's execution, the system tracks any intended changes to be made during the transaction in the transaction's \emph{write set} and any state observed during the transaction in the transaction's \emph{read set}. When the transaction attempts to commit, the system checks whether the observed values in the read set are still valid. If so, the system performs the changes in the transaction's write set and the transaction is marked committed. Else the transaction aborts and the system ensures that the transaction leaves no visible effects.

Transactional memory can be implemented in both hardware and software. Hardware transactional memory (HTM) naturally outperforms software TMs, but a purely hardware TM has capacity restraints and fails to be truly composable\cite{htm}. The STO system\cite{sto} is one of several developed software transactional memory (STM) systems. TL2\cite{tl2} and LarkTM\cite{larktm} are highly-optimized word-STMs that track memory words touched during a transaction. SwissTM\cite{swisstm} is also a word-STM, but increases performance by tracking memory in 4-word groups, resulting in less overhead than tracking individual memory words. There has also been significant research in non-transactional APIs (open nesting\cite{opennesting}, elastic transactions\cite{elastic}, transactional collection classes\cite{tcc}, early release\cite{earlyrelease}, SpecTM\cite{spectm}); these allow for programmers to reduce bookkeeping costs and false conflicts between transactions, but also require programmers to be experts in the system to implement such optimizations. This complicates, rather than simplifies, concurrent programming. There have also been object-based STMs which track objects instead of memory words, but incur extra cost by shadow copying any objects written to within the transaction. STO is one of several STM systems that use abstraction to improve TM performance\cite{boost}\cite{optboost}\cite{autolock}\cite{predication}. These systems expose a transactional API to programmers in the form of concurrent data structures; these data structures are written on top of an STM. However, these systems build their data structures on top of traditional word-STMs, whereas STO builds data structures on top of an abstract STM which tracks abstract items defined by each data structure. This lets STO to improve performance beyond that of previous systems. Many STMs, including STO, are now looking to improve performance by integrating STM with HTM.

\subsection{Abstract Data Types in STM}
Our work focuses on data types and how they perform within transactional settings. We now take a closer look at other abstract STM systems that expose an API in the form of transactional data structures that allow programmers to work with transactions.

Boosting\cite{boost} is a method to convert high-concurrency (linearizable) data structures implemented without any notion of transaction into transactional data structures. Like STO, boosting determines conflicts between transactions by relying on particular data structure semantics; however, boosting determines conflicts by mapping a data structure's operations to abstract locks. If two operations do not commute---i.e. swapping the order of their invocations affects the final state of the data structure or the responses returned by the operations---then the abstract locks for the operations will conflict. For one of the operations to be performed, both of the abstract locks must be acquired. Thus, the granularity of synchronization of the original linearizable data structure is only achievable in a boosted data structure for commutative operations. Because a transaction may abort, boosting practices undo logging and requires that each operation has an inverse. These constraints limit the applications of boosting to all data structures, but make it particularly useful for ones like sets. Unlike boosting, STO allows us to work with data structures whose operations have no inverse. STO also allows for more flexibility in its data structure algorithms: we can work with nonlinearizable concurrent data structures and directly alter the concurrent data structure's algorithm to work with transactions.

Optimistic boosting\cite{optboost} improves the performance of boosting by proposing an optimistic methodology in acquisition of the abstract locks is delayed until commit time. Instead, like STO, semantic items are added to the read and writes sets of the transaction and validated at commit time; if all reads are valid, then the appropriate abstract locks are acquired and modifications applied to the data structure. Because execution is optimistic and abstract locks are not eagerly acquired at the higher, ``boosted'' level, the underlying concurrent data structure can be lazy. This allows for un-monitored traversals during execution time to improve performance, as well as support for operations that may not have an inverse. However, STO provides a more general transactional framework (for example, allowing for a mix of pessimistic and optimistic approaches in data structure design) and achieves larger performance gains that optimistic boosting. 

Automated locking\cite{autolock} takes a similar approach to boosting in that it also pessimistically acquires abstract locks corresponding to each operation. It differs from boosting because it also takes a commutativity specification (of conditions in which operations commute) that allows an abstract data type to compiles into symbolic sets (locking modes) that are used to lock operations. These locking modes allow two commutative operations to run concurrently. Thus, it is able to optimize and automate the creation of optimize abstract locks for an abstract data structure. A similar approach to automated locking may help STO data structures choose which abstract read/write items to track during a transaction to avoid false conflicts.

Predication\cite{predication} is a technique that maps element to predicates and uses an STM to manage access to the predicate. For example, an element in a set would be associated with an \texttt{in\_set?}\ predicate and the STM would manage reads and writes of the predicate when elements are removed/added to the set. Unlike boosting, transactional predication achieves semantic conflict detection without keeping an undo log. However, STO does not face the same garbage collection problems as predication, since predication must create predications for absent as well as present elements. Transactional predication also focuses on making transactional data structures perform equally as well as high-concurrency data structures in non-transactional settings. This is orthogonal to our work here and can be a future line of optimization for STO data structures.

Transactional Data Structures Libraries\cite{tdsl}, like STO, aims to produce libraries of transactional data structures. Similar to STO, TDSL customizes (potentially different) STM strategies for each individual data structure and provides optimizations that rely on the data structure's semantics. Unlike STO, data structures in TDSL are optimized for single-operation transactions (which we see as an orthogonal line of work). While the example TDSL contains only a queue and skiplist, STO has implementations of many other data structures in transactional settings, including a queue, hashmap, list, priority queue, and red-black tree. Our work in this thesis draws upon some of the algorithm designs for the queue implemented in TDSL.

Our algorithms for STO data structures combines work done in concurrent data structure design, including locking, optimistic verification, direct updates (eager versioning) and deferred updates (lazy versioning). This thesis investigates several concurrent data structures. In particular, we test and modify different concurrent queue \cite{queue1}\cite{queue2}\cite{queue3}\cite{queue4}\cite{queue5}\cite{flatcombining} and concurrent hashmap\cite{hm1}\cite{hm2}\cite{hm3}\cite{chm} algorithms, which we describe in more detail in sections \ref{Queue} and \ref{HashMap}. Similar work has been done with lazy sets\cite{lazyset}; their work adds transactional support to a lazy concurrent set. We extend their work to other data structures and investigate further into how to achieve performance close to highly concurrent (non-transactional) data structure.

\subsubsection{Commutativity in Transactional Data Structures}

STO, along with the above methods of integrating concurrent abstract data types with STM, builds upon the ideas introduced by Weihl in the late 1980s\cite{weihl}. Weihl defines optimal local atomicity properties of data types that allow for transactions to be serialized, therefore providing an upper bound on the amount of concurrency achievable in transactional data types. These local properties are derived from algebraic properties of the data structure, such as commutativity of particular operations. Weihl also demonstrates an inherent relation between commutativity-based concurrency control and transactional recovery algorithms. Unlike STO, however, Weihl does not focus on the implementation of these properties.

Schwarz and Spector\cite{schwarz} introduce a theory to order concurrent transactions based on the semantics of shared abstract (data)types and dependencies of different datatype operations. In addition to pre/postconditions and invariants of operations, they introduce the concept of an interleaving specification to describe how operations within transactions can be interleaved. Our analysis of the transactional FIFO Queue builds off their work with abstract queue schedule and dependencies. Our work with the weakly-transactional queue is similar to their WQueue (a higher-concurrency queue with modified semantics), but they focus on the theoretical result instead of the implementation and do not concern themselves with explicit concurrency algorithms for the queue. As in the boosting technique, Schwarz and Spector describe synchronization algorithms for the data types in terms of abstract locks.

Badrinath and Ramamritham\cite{badrinath} also work with operation commutativity to define the notion of a recoverable operation; this helps them achieve enhanced concurrency. Commutativity has also played a role in network consistency algorithms and CDRTs\cite{CRDT}, as well as in the database community (where the idea of a transaction has, of course, been heavily researched).
\lyt{Korth?}

Kulkarni, et.al.\cite{galois} define the notion of a commutativity lattice (predicates between pairs of methods) to reason about commutativity in a data structure. The Galois system upon which this idea is tested provides a framework in which the programmer defines a commutativity lattice for individual data structures, and, by exploiting commutativity, improves the performance of irregular parallel applications. Galois, however, focuses on constructing commutativity checkers instead of serializing transactions.
\lyt{expand?}
